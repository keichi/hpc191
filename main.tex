\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{cite}
\usepackage[japanese]{babel}
\usepackage[scaled]{beramono}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage[dvipdfmx]{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[all, warning]{onlyamsmath}
\usepackage{siunitx}
\usepackage[subrefformat=parens]{subcaption}
\usepackage{url}

\newcommand{\us}{\si{\micro\second}}

\begin{document}


\title{ベクトル型スーパーコンピュータ「AOBA-S」の性能評価}

\etitle{Performance Evaluation of a Vector Supercomputer ``AOBA-S''}

\affiliate{CSC}{東北大学サイバーサイエンスセンター}
\affiliate{NEC}{日本電気株式会社}
\affiliate{TU}{東北大学大学院情報科学研究科}
\affiliate{TDU}{東京電機大学}

\author{高橋 慧智}{Keichi Takahashi}{CSC,TU}[keichi@tohoku.ac.jp]
\author{藤本 壮也}{Soya Fujimoto}{NEC}[s-fujimoto@nec.com]
\author{長瀬 悟}{Satoru Nagarase}{NEC}[s.nagase@nec.com]
\author{磯部 洋子}{Yoko Isobe}{NEC}[y-isobe-pi@nec.com]
\author{下村 陽一}{Yoichi Shimomura}{CSC,TU}[shimomura32@tohoku.ac.jp]
\author{江川 隆輔}{Ryusuke Egawa}{TDU}[egawa@mail.dendai.ac.jp]
\author{滝沢 寛之}{Hiroyuki Takizawa}{CSC,TU}[takizawa@tohoku.ac.jp]

\begin{abstract}
東北大学サイバーサイエンスセンターは，2023年8月よりベクトル型スーパーコンピュータ「AOBA-S」の運用を
開始した．AOBA-Sは第3世代ベクトルエンジン (VE30) をノードあたり8基搭載した計504ノードから構成され，
理論演算性能は21.05\,PFLOP/s，メモリ帯域幅は9.97\,PB/sに達する世界最大規模のベクトル型
スーパーコンピュータである．本稿ではAOBA-Sの設計を概説し，運用開始に先駆けて実施したAOBA-Sの
初期性能評価の結果について報告する．
\end{abstract}

\begin{eabstract}
Cyberscience Center, Tohoku University has started the operation of a new vector supercomputer 
``AOBA-S'' in August 2023. AOBA-S comprises 504 compute nodes each equipped with eight
third-generation Vector Engine (VE30) cards. The peak compute performance and memory bandwidth of
AOBA-S reach 21.05\,PFLOP/s and 9.97\,PB/s, respectively, making it the world's largest vector
supercomputer as of writing this paper. In this paper, we describe the basic design of AOBA-S and
report the results of the initial performance evaluation that we have conducted prior to the
operation start of AOBA-S.
\end{eabstract}

%
%\begin{jkeyword}
%情報処理学会論文誌ジャーナル，\LaTeX，スタイルファイル，べからず集
%\end{jkeyword}
%
%\begin{ekeyword}
%IPSJ Journal, \LaTeX, style files, ``Dos and Dont's'' list
%\end{ekeyword}

\maketitle

\section{はじめに}

\section{AOBA-Sのアーキテクチャ}

\subsection{SX-Aurora TSUBASA C401-8}

図\ref{fig:node}にAOBA-Sを構成するSX-Aurora TSUBASA (SX-AT) C401-8ノードの概念図を示す．
SX-Aurora TSUBASAはVector Host (VH) とVector Engine (VE) の2種のプロセッサが搭載した
ヘテロジニアスな計算機である．VEはPCI Expressカード上に実装されたベクトルプロセッサであり，HBMと
密結合されたことにより．VHはVEから発行されたI/O等のOS処理を担うためのx86\_64プロセッサである．
SX-AT C401-8はVHとして64コアのAMD EPYC 7763 CPUを採用しており，各ノードに8基のVector Engine Type
30Aカードを搭載している．8基のVEは2基ずつ共有するPCIeスイッチを経由し，VHに接続している．
VEは1基あたり96\,GBのHBM2Eメモリ，VHは256GBのDDR4メモリを搭載している．また，VHには2基のInfiniBand
NDR 200G HCAが搭載されており，VHおよびVE間の通信手段として利用することが可能である．

\begin{figure}
  \centering
  \includegraphics{figs/node_arch.pdf}
  \caption{SX-Aurora TSUBASA C401-8の構成}\label{fig:node}
\end{figure}

図\ref{fig:ve30}にVector Engine Type 30A (VE30)プロセッサの概要を示す．VE30はVector
Engineプロセッサの第3世代となるプロセッサである．VE30は16個のベクトルコアを搭載し，これらの
ベクトル・コアが2次元メッシュ型Network on Chip
(NoC)によって接続されている．ベクトルコア群はNoCによって6.4\,TB/sの帯域幅を有するLast Level Cache 
(LLC) に接続しており，さらにLLCは2.45\,TB/sの帯域幅を有する96\,GBのHBM2Eメモリに接続している．
各ベクトルコアはScalar Processing Unit (SPU) とVector Processing Unit (VPU) という2種の実行ユニット
を搭載している．SPUは命令をフェッチ，デコードし，VPUへベクトル命令を発行する．VPUはベクトル命令を
実行する．SPUにはL1キャッシュとL2キャッシュが搭載されており，さらに，VE30ではSPUとVPUが共有する
2\,MBのL3キャッシュが追加されている．

\begin{figure}
  \centering
  \includegraphics{figs/ve30_memory_hierarchy.pdf}
  \caption{VE30の概要図~\cite{Takahashi2023}}\label{fig:ve30}
\end{figure}

前世代のVE Type 20Aと比較すると，ベクトルコアあたりの計算性能とメモリアクセス性能を同一である一方，
コア数が10コアから16コアへ1.6倍に増加している．また，コア数の増加に応じてメモリ帯域幅も1.53\,TB/sから
2.54\,TB/sへ1.6倍に向上している．さらに，メモリ容量は48\,GBから96\,GBへ2倍に拡大され，LLCの容量は
16\,MBから64\,MBへ4倍に拡大している．

\subsection{AOBA-S}

\begin{table}
\centering
\caption{AOBA-Sの性能諸元}\label{tbl:aoba-s}
\begin{tabular}{@{}lll@{}}
\toprule
ノード数        & ノード単体     & システム全体           \\ \midrule
VE数            & 8              & 4,032                  \\ \midrule
VH数            & 1              & 504                    \\
VE理論演算性能  & 39.28\,TFLOP/s & 19.79\,PFLOP/s         \\
VEメモリ帯域幅  & 19.60\,TB/s    & 9.87\,PB/s             \\
VEメモリ容量    & 768\,GB        & 378\,TB                \\ \midrule
VH理論演算性能  & 2.50\,TFLOP/s  & 1.26\,PFLOPS/s         \\
VHメモリ帯域幅  & 0.20\,TB/s     & 0.1\,PB/s              \\
VHメモリ容量    & 256\,GB        & 126\,TB                \\ \midrule
相互結合網      & \multicolumn{2}{c}{InfiniBand NDR}      \\
共有ストレージ  & \multicolumn{2}{c}{Lustre 4.4\,PB}      \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}
  \centering
  \includegraphics[width=.9\columnwidth]{figs/rack.jpg}
  \caption{AOBA-Sの外観}\label{fig:rack}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.9\columnwidth]{figs/ve30.jpg}
  \caption{Vector Engineカードの外観}\label{fig:rack}
\end{figure}

AOBA-Sは，504ノードのSX-Aurorao TSUBASA C401-8から構成されるスーパーコンピュータである．

1ラックあたり16ノードが搭載されており，計32ラックから構成される．
相互結合網にはInfiniBand
NDRを採用しており，フルバイセクションバンド幅およびノンブロッキングの2段Fat-treeトポロジによって
計算ノードおよびストレージノードが接続されている．
2基のスパインスイッチおよび
計算ノード側のHCAにはInfiniBand NDR
200GのHCAが2枚搭載されている．計算ノード群2ラックにつき1つのInfiniBandスイッチに接続している．
計算ノードのバイセクションバンド幅は，204.8\,Tbpsに達する．

\begin{figure}
  \centering
  \includegraphics{figs/nw_topology.pdf}
  \caption{Vector Engineカードの外観}\label{fig:rack}
\end{figure}

ストレージにはDDN社製ES400NVX2アプライアンスを採用している．
1基のES400NVX2がLustre MDS/MDTとして動作し，もう1基のES400NVX2がOSS/OSTとして動作する．
OSS/OST用ES400NVX2には4基のSS9012エンクロージャが接続され，合計332本の7200rpm ニアラインSAS
HDDを搭載している．各ES400NVX2は8本のInfiniBand HDRリンクによって相互結合網に接続している．

\section{HPL・HPCG性能}

High Performance Linpack (HPL)~\cite{Dongarra2003}ベンチマークおよびHigh Performance Conjugate
Gradients (HPCG)~\cite{Dongarra2016} ベンチマークを実行した．
504 VHの全系実行では，HPLの性能が16.33\,PFLOP/s (実行効率82.4\%)，HPCG性能が913.1\,TFLOP/s 
(実行効率4.61\%) なお，これらの性能はパラメータのチューニング等の最適化を実施する前の性能であり，
今後チューニングを実施することにより，さらに性能が向上する予定である．

\begin{figure}
  \centering
  \includegraphics{figs/hpl_hpcg.pdf}
  \caption{HPLおよびHPCG性能}\label{fig:hpl-hpcg}
\end{figure}

\section{MPI通信性能}

OSU Micro-Benchmarks
7.2\footnote{\url{https://mvapich.cse.ohio-state.edu/benchmarks/}}を用いてMPI通信の性能を計測した．
NCC 5.0.1でコンパイルし，MPIライブラリにNEC MPI 3.4.0を用いた．
測定にあたっては，(1) 同一PCIeスイッチに接続されたVE間，(2)
同一ノード内の異なるPCIeスイッチに接続されたVE間，(3) 同一ラックの異なるノードに接続されたVE間，
の3パターンを計測した．

\subsection{1対1通信}

図\ref{fig:mpi-lat}に\verb|osu_latency|ベンチマークで計測したMPI 1対1通信のレイテンシを示す．
同一ノード内の同一PCIeスイッチに接続されたVE間の最低レイテンシは1.51\us であった．
同一ノード内の別PCIeスイッチに接続されたVE間ではルートコンプレックスを経由する必要があり，
レイテンシは1.88\us であった．
ノード間 3.87\us
ノード間 2.09\us

\begin{figure}
  \centering
  \includegraphics{figs/mpi_latency.pdf}
  \caption{MPI 1対1通信のレイテンシ}\label{fig:mpi-lat}
\end{figure}

図\ref{fig:mpi-lat}に\verb|osu_bandwidth|ベンチマークで計測したMPI 1対1通信のスループットを示す．

PCIe Gen 4 $\times$16の実効帯域幅31.508\,GB/sである．73.4\%

\begin{figure}
  \centering
  \includegraphics{figs/mpi_bandwidth.pdf}
  \caption{MPI 1対1通信のスループット}\label{fig:bw}
\end{figure}

\subsection{集団通信}

\section{ストレージ性能}

IOR 3.3.0を用いて並列ファイルシステムのスループットを計測した．1VEにつきIORを1プロセスを起動し，
1プロセスあたり1ファイルに書き込む設定で計測を実施した．

計測に用いたコマンドは，
\texttt{ior -w -r -a POSIX -F -Q 8 -C -e -g -b 16m -t 4m -s 250 -i 3}である．
書き込み時のページキャッシュの効果を排除するため，\texttt{-e}オプションによってwrite完了時に
\texttt{fsync()}を呼び出している．また，読み込み時のページキャッシュの効果を排除するため，書き込み
プロセスと読み込みプロセスをずらす\texttt{-C}オプションを指定している．また，同一VHに接続された
VEはVH側においてページキャッシュを共有するため，書き込みVEと読み込みVEがそれぞれ異なるVHに所属する
ように\texttt{-Q 8}オプションによって書き込みと読み込みプロセスを8ランク分ずらしている．

図\ref{fig:ior}に計測結果を示す．1024 VEにおいて書き込み49.2\,GB/s，読み込み39.0\,GB/sのスループット
を達成した．

\begin{figure}
  \centering
  \includegraphics{figs/ior.pdf}
  \caption{並列ファイルシステムのスループット}\label{fig:ior}
\end{figure}

\section{SPEChpc}

\subsection{Tinyサイズ}

\subsection{Largeサイズ}

\subsection*{謝辞}

性能評価にご協力いただいた東北大学情報部デジタルサービス支援課および日本電気株式会社の皆様に
感謝いたします．

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
