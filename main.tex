\documentclass[submit,techrep,noauthor]{ipsj}

\usepackage{amsmath,amssymb,amsfonts}
\usepackage{cite}
\usepackage[japanese]{babel}
\usepackage[scaled]{beramono}
\usepackage{booktabs}
\usepackage{caption}
\usepackage[T1]{fontenc}
\usepackage[dvipdfmx]{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[all,warning]{onlyamsmath}
\usepackage{siunitx}
\usepackage[subrefformat=parens]{subcaption}
\usepackage{url}

\begin{document}

\title{ベクトル型スーパーコンピュータ「AOBA-S」の性能評価}

\etitle{Performance Evaluation of a Vector Supercomputer ``AOBA-S''}

\affiliate{CSC}{東北大学サイバーサイエンスセンター}
\affiliate{NEC}{日本電気株式会社}
\affiliate{TU}{東北大学大学院情報科学研究科}
\affiliate{TDU}{東京電機大学}

\author{高橋 慧智}{Keichi Takahashi}{CSC,TU}[keichi@tohoku.ac.jp]
\author{藤本 壮也}{Soya Fujimoto}{NEC}[s-fujimoto@nec.com]
\author{長瀬 悟}{Satoru Nagaase}{NEC}[s.nagase@nec.com]
\author{磯部 洋子}{Yoko Isobe}{NEC}[y-isobe-pi@nec.com]
\author{下村 陽一}{Yoichi Shimomura}{CSC}[shimomura32@tohoku.ac.jp]
\author{江川 隆輔}{Ryusuke Egawa}{TDU}[egawa@mail.dendai.ac.jp]
\author{滝沢 寛之}{Hiroyuki Takizawa}{CSC,TU}[takizawa@tohoku.ac.jp]

\begin{abstract}
東北大学サイバーサイエンスセンターは，2023年8月よりベクトル型スーパーコンピュータ「AOBA-S」の運用を
開始した．AOBA-Sは第3世代ベクトルエンジン (VE30) をノードあたり8基搭載した計504ノードから構成され，
理論演算性能は21.05\,PFLOP/s，メモリ帯域幅は9.97\,PB/sに達する世界最大規模のベクトル型
スーパーコンピュータである．本稿ではAOBA-Sの設計を概説し，運用開始に先駆けて実施したAOBA-Sの
初期性能評価の結果について報告する．
\end{abstract}

\begin{eabstract}
Cyberscience Center, Tohoku University has started the operation of a new vector supercomputer 
``AOBA-S'' in August 2023. AOBA-S comprises 504 compute nodes each equipped with eight
third-generation Vector Engine (VE30) cards. The peak compute performance and memory bandwidth of
AOBA-S reach 21.05\,PFLOP/s and 9.97\,PB/s, respectively, making it the world's largest vector
supercomputer as of writing this paper. In this paper, we describe the basic design of AOBA-S and
report the results of the initial performance evaluation that we have conducted prior to the
operation start of AOBA-S.
\end{eabstract}

\maketitle

\section{はじめに}

東北大学サイバーサイエンスセンター (以下本センター) は，その前身である東北大学大型計算機センターが
1969年に設置されて以来スーパーコンピュータを運用し，計算機を活用した最先端の学術研究に貢献してきた．
本センターは全国の研究機関の研究者が学術研究のために利用する全国共同利用施設であると同時に，
全国の情報基盤センター群と連携した学際大規模情報基盤共同利用・共同研究拠点 (JHPCN) の構成拠点である．
また，「富岳」を中核とする全国の計算機資源を連携した革新的ハイパフォーマンス・コンピューティング・
インフラ (HPCI) の構成機関でもあり，共同研究活動を推進する拠点となっている．

このような背景の下，本センターでは2020年10月にベクトル型スーパーコンピュータ「AOBA-A」および
スカラ型並列コンピュータ「AOBA-B」を導入し，高速で大規模な計算処理環境を提供してきた．
両サブシステムの利用率は常に高く，2022年10月にはクラウド上システムAOBA-Cによって処理能力の一時的な
増強を実施したものの，計算需要が処理能力を超える状況が続いてきた．
そこで，本センターでは次期スーパーコンピュータ「AOBA-S」の構想・設計を進め，
2022年4月に入札公示を行った．2022年8月に日本電気株式会社が落札し，
2023年8月より利用者へのサービス提供を開始した．

AOBA-SはAOBA-Aと比較すると14倍以上の理論演算性能と11倍以上のメモリ帯域幅を有し，
2023年8月現在では我々の知る限り，世界最大規模かつ世界最高性能のベクトル型スーパーコンピュータである．
AOBA-Sの中核を成すVector Engine Type 30AプロセッサはVector Engineプロセッサファミリの第3世代であり，
第2世代に比べると演算性能やメモリ帯域幅等の基本性能の向上に加え，様々なアーキテクチャの改良が
施されている．そのため，実アプリケーションでは理論演算性能やメモリ帯域幅の向上を超える
性能向上が期待できる．
本稿では，AOBA-Sの運用開始に先駆けて実施した各種性能評価の結果について報告する．

\section{スーパーコンピュータ「AOBA-S」}

\subsection{概要}

\begin{figure}[tb]
  \centering
  \includegraphics[width=.9\columnwidth]{figs/rack.jpg}
  \caption{AOBA-Sの外観}\label{fig:aoba-s}
\end{figure}

AOBA-Sの外観を図\ref{fig:aoba-s}に示す．
AOBA-Sは，504ノードのNEC製SX-Aurora TSUBASA C401-8システムから構成されるスーパーコンピュータである．
図\ref{fig:node}にSX-Aurora TSUBASA C401-8の構成を示す．
SX-Aurora TSUBASAはVector Host (VH) とVector Engine (VE) の2種のプロセッサが搭載された
ヘテロジニアスな計算機である．VEはPCI Expressカード上に実装されたベクトルプロセッサであり，
アプリケーションの実行を担う．VEは2,048ビット幅のベクトル演算命令を備え，
高帯域幅メモリとの密結合により，メモリ律速のアプリケーションにおいて高い性能を発揮する．
VHはVEの制御を担い，VEから発行されたI/O等のOS処理を実行するx86\_64プロセッサである．
C401-8はVHとして64コアのAMD EPYC 7763 CPUを採用しており，各ノードに8基のVector Engine Type
30Aカードを搭載している．8基のVEはPCIe Gen 4.0 x16によって
2基ずつ共有するPCIeスイッチを経由し，VHに接続している．
VEは1基あたり96\,GBのHBM2Eメモリを，VHは256\,GBのDDR4メモリを搭載している．また，VHには2基のInfiniBand
NDR200 HCAが接続されており，VHおよびVE間で高速通信を実現している．

表\ref{tbl:aoba-s}にAOBA-Sの1ノードとシステム全体の性能諸元を示す．
8基のVEが計39.28\,TFLOP/s，1基のVHが2.50\,TFLOP/sの演算性能を
有するため，1ノードあたり計41.78\,TFLOP/sの演算性能を有する．
504ノードのシステム全体では21.05\,PFLOP/sの演算性能となる．
メモリ帯域については同様にノードあたり19.8\,TB/s，システム全体では9.97\,PB/sである．
これらの性能値を2023年6月版Top500リスト\footnote{\url{https://www.top500.org/lists/top500/2023/06/}}
に掲載されている国内のシステムと比較すると，
理論演算性能においては「Wisteria/BDEC-01」に次いで国内4位，メモリ帯域幅においては「富岳」に次いで
国内2位の性能である．

\begin{table}[tb]
\centering
\caption{AOBA-Sの性能諸元}\label{tbl:aoba-s}
\begin{tabular}{@{}lll@{}}
\toprule
ノード数        & ノード単体     & システム全体           \\ \midrule
VE数            & 8              & 4,032                  \\ \midrule
VH数            & 1              & 504                    \\
VE理論演算性能  & 39.28\,TFLOP/s & 19.79\,PFLOP/s         \\
VEメモリ帯域幅  & 19.60\,TB/s    & 9.87\,PB/s             \\
VEメモリ容量    & 768\,GB        & 378\,TB                \\ \midrule
VH理論演算性能  & 2.50\,TFLOP/s  & 1.26\,PFLOPS/s         \\
VHメモリ帯域幅  & 0.20\,TB/s     & 0.1\,PB/s              \\
VHメモリ容量    & 256\,GB        & 126\,TB                \\ \midrule
相互結合網      & \multicolumn{2}{c}{InfiniBand NDR}      \\
共有ストレージ  & \multicolumn{2}{c}{Lustre 4.4\,PB}      \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}[tb]
  \centering
  \includegraphics{figs/node_arch.pdf}
  \caption{SX-Aurora TSUBASA C401-8の構成}\label{fig:node}
\end{figure}

\subsection{Vector Engine Type 30A (VE30)}

% \begin{figure}[tb]
%   \centering
%   \includegraphics[width=.9\columnwidth]{figs/ve30.jpg}
%   \caption{Vector Engine Type 30Aの外観}\label{fig:ve-card}
% \end{figure}

% 図\ref{fig:ve-card}にVector Engine Type 30A (VE30) カードの外観を，
図\ref{fig:ve30}にVE30プロセッサの概要を示す．VE30はVector
Engineプロセッサファミリの第3世代となるプロセッサである．VE30は16個のベクトルコアを搭載し，これらの
ベクトルコアが2次元メッシュ型Network on Chip (NoC)によって接続されている．
各ベクトルコアは倍精度307.2\,GFLOP/s，単精度614.4\,GFLOP/sの演算性能を有し，プロセッサ全体では
倍精度4.91\,TFLOP/s，単精度9.83\,TFLOP/sの演算性能を有する．
ベクトルコア群はNoCによって6.4\,TB/sの帯域幅を有するLast Level Cache 
(LLC) に接続しており，さらにLLCは2.45\,TB/sの帯域幅を有する96\,GBのHBM2Eメモリに接続している．

各ベクトルコアはScalar Processing Unit (SPU) とVector Processing Unit (VPU) と呼ばれる2種の実行ユニット
を備えている．SPUは命令をフェッチ，デコードし，スカラ命令を実行するほか，
ベクトル命令をVPUへディスパッチする．VPUはベクトル命令を実行する．
SPUには64\,KBのL1キャッシュと512\,KBのL2キャッシュが搭載されている．
さらに，VE30ではSPUとVPUが共有する2\,MBのプライベートL3キャッシュが新設されている．
L3キャッシュはメモリアクセスレイテンシを短縮する効果のほか，NoCおよびLLCにおける競合を
軽減することから，キャッシュ帯域幅律速のアプリケーションの性能を向上する効果も期待される．
また，L3キャッシュはソフトウェアによってバイパスすることが可能であり，
局所性の高いデータと低いデータが混在するアプリケーションにおいて選択的キャッシングを行うことで
限られたキャッシュ容量を有効活用できる．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/ve30_memory_hierarchy.pdf}
  \caption{VE30の概要図~\cite{Takahashi2023}}\label{fig:ve30}
\end{figure}

VE30を前世代のVE Type 20Aと比較すると，コアあたりの演算性能とメモリアクセス性能は同一である一方，
コア数が10コアから16コアへ1.6倍に増加し，プロセッサ全体の演算性能は1.6倍に向上している．
また，コア数の増加に応じてメモリ帯域幅も1.53\,TB/sから
2.54\,TB/sへ1.6倍に向上している．さらに，メモリ容量は48\,GBから96\,GBへ2倍に拡大し，LLC容量は
16\,MBから64\,MBへ4倍に拡大している．LLCの帯域幅は3.0\,TB/sから6.4\,TB/sに2.13倍に向上している．

これらの性能向上に加え，VE30ではリストベクトル演算を加速するための専用ハードウェアの新設や，
単精度演算におけるアラインメント制約の緩和など，アーキテクチャに様々な改良が加えられている．
これらの改良点の性能分析については文献~\cite{Takahashi2023}を参照されたい．

\subsection{相互結合網とストレージ}

AOBA-Sを構成する装置群は計40基のサーバラックに収容されている．
計算ノードについては1ラックに16 VHが搭載されており，32ラックに計504 VHが収容されている．
これらに加え，フロントエンドノードおよび管理・監視サーバ群が2ラック，ネットワーク機器が2ラック，
無停電電源装置が3ラック，ストレージが1ラックの計8ラックに収容されている．

相互結合網にはInfiniBand NDRを採用しており，図\ref{fig:topo}に示すように，
フルバイセクション帯域幅およびノンブロッキングの2段Fat-treeトポロジによって計算ノード，
ストレージ，およびフロントエンドノードが接続されている．相互結合網は16基のスパインスイッチおよび
18基のリーフスイッチによって構成される．各InfiniBandスイッチにはNVIDIA Quantum-2 QM9700を採用している．
リーフスイッチの内16基には計算ノードが接続されており，
残りの2基にはストレージおよび各種サーバ群が接続されている．計算ノードが接続されている
各リーフスイッチには32 VH (64 HCA) が接続されている．各スパインスイッチとリーフスイッチの間は2本の
400\,Gb/sリンクによって接続されており，計算ノード部分のバイセクション帯域幅は
% 400\,Gb/s$\times$2$\times$16$\times$16=204.8\,Tb/sに達する．
$400\text{\,Gb/s} \times 2 \times 16 \times 16 = 204.8\text{\,Tb/s}$に達する．

\begin{figure}[tb]
  \centering
  \includegraphics[scale=0.9]{figs/nw_topology.pdf}
  \caption{AOBA-Sの相互結合網}\label{fig:topo}
\end{figure}

ストレージにはDDN社製LustreアプライアンスであるES400NVX2を2基採用している．
1基のES400NVX2がMDS/MDT機能を提供しており，仮想マシンとして4台のMDSが稼働している．
MDTの実効容量は15.3\,TBであり，格納可能な最大i-node数は約55億である．
もう1基のES400NVX2はOSS/OST機能を提供しており，仮想マシンとして4台のOSSが稼働している．
OSS/OST用ES400NVX2には4基のSS9012エンクロージャが接続され，計332本の7.2k\,rpm NL-SAS
HDDを搭載している．各ES400NVX2は8本のInfiniBand HDR100リンクによって相互結合網に接続している．
本ストレージは利用者毎の\verb|/uhome|領域として提供しているほか，
同一課題の参加者間でファイルを共有するための\verb|/short|領域も提供している．
また，各ノードには1\,TBのSATA SSDが搭載されており，高速一時領域\verb|/SSDTMP|として利用可能である．

\subsection{ソフトウェア環境}

利用者のプログラム開発環境として，高度な自動ベクトル化および自動並列化機能を備える
NEC C/C++コンパイラとNEC Fortranコンパイラを提供している．これらのコンパイラはそれぞれ
C++ 14/17とFortran 2008に準拠している．また，GNUツールチェーンの各種ツールがVE用に移植されており，
プロファイラとしてngprof，デバッガとしてve-gdbが利用可能である．さらに
プログラムの
ベクトル化率，平均ベクトル長，キャッシュヒット率等をハードウェアパフォーマンスカウンタによって
計測するためのツールとしてPROGINF，同様の情報を関数単位で計測するためのツールとしてFTRACEを
提供している．

数値計算ライブラリ集としてはVector Engine向けに最適化されたNEC Numeric Library Collection (NLC) を
提供している．NLCはBLAS，LAPACK，SBLAS，ScaLAPACK，FFTW等のライブラリと互換の
インターフェースを備えており，既存ソフトウェア資産の移植を容易にしている．

\section{性能評価}

本節では，AOBA-Sのプロセッサおよびシステム全体の多面的な性能評価結果について報告する．
まず，\ref{sec:proc}節においてVE30プロセッサの単体性能を報告し，\ref{sec:hpl-hpcg}節において
全系を用いたHPLおよびHPCGベンチマークの性能を報告する．
\ref{sec:mpi}節と\ref{sec:storage}節においてそれぞれMPI通信性能とストレージ性能について報告する．
最後に，\ref{sec:spechpc}節においてSPEChpcベンチマークを用いた実アプリケーションを想定した
性能評価の結果を報告する．

\subsection{プロセッサ単体性能}\label{sec:proc}

VE30プロセッサの単体性能を同世代の他プロセッサと比較する．
比較対象のプロセッサは，第2世代VEの8コアモデルであるVector Engine Type 20B，富士通A64FX，
Intel Xeon Platinum 8368 (IceLake-SP)，NVIDIA A100 PCIe 40\,GBおよび80\,GBモデルである．
% 各プロセッサの詳細な諸元については，文献~\cite{Takahashi2023}を参照されたい．

\subsubsection{基本性能}

まず，業界標準のベンチマークであるBabelStream~\cite{Deakin2018}，
High Performance Linpack (HPL)~\cite{Dongarra2003}，およびHigh Performance Conjugate
Gradients (HPCG)~\cite{Dongarra2016}を用いて基本性能を評価する．
HPLおよびHPCGのバイナリには，
各プロセッサのベンダより提供されている最適化されたバイナリを使用した．なおA64FXについてはバイナリを
入手できなかったため，Top500に掲載されている全系実行の性能より推定した．

図\ref{fig:stream}にBabelStream v4.0によって計測した各プロセッサの実効メモリ帯域幅を示す．
VE30の実効メモリ帯域幅は1.79\,TB/sとなり，他の全てのプロセッサより高い性能となった．
一方，ピークメモリ帯域幅に対する効率は72\%であり，これはA100 40\,GBモデルの91\%や80\,GBモデルの
86\%に比べると低い．これは，VEのキャッシュラインサイズが128\,Bであるのに対し，GPUの
キャッシュラインサイズは256\,Bと2倍の大きさであるため，STREAMのような連続メモリアクセスにおいて
GPUが有利であるためである．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/stream.pdf}
  \caption{BabelStream性能~\cite{Takahashi2023}}\label{fig:stream}
\end{figure}

図\ref{fig:hpl}にHPLベンチマークの性能を示す．HPLは演算律速なベンチマークであるため，理論演算性能が
高いA100 40\,GBモデルと80\,GBモデルの実効性能がそれぞれ11.8\,TFLOP/sと12.5\,TFLOP/sと非常に高い．
一方，実行効率は60\%台とVEやA64FXに比べると低い．
\verb|nvidia-smi|コマンドでベンチマークの実行中にGPUの動作周波数を監視すると周波数が低下していたため，
PCIe版では電力供給が不足し，パワースロットリングによって性能低下していると考えられる．
VE30の性能は4.43\,TFLOP/sとなり，A100の両モデルに次いで最も高くなった．
実行効率は90\%と高く，\verb|vecmd|\footnote{\url{https://sxauroratsubasa.sakura.ne.jp/documents/guide/pdfs/InstallationGuide_E.pdf}}
コマンドで周波数を監視したところ，パワースロットリングは発生していなかった．よって，VE30は
PCIeカードの電力制約内で安定して性能を発揮できている．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/hpl.pdf}
  \caption{HPL性能~\cite{Takahashi2023}}\label{fig:hpl}
\end{figure}

図\ref{fig:hpcg}にHPCGベンチマークの性能を示す．VE30におけるHPCGベンチマークの性能は258\,GFLOP/sであり，
A100 80\,GBモデルの259\,GFLOP/sとほぼ同等の結果となった．実行効率は5.2\%であり，A100 80\,GBモデルの
2倍となった．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/hpcg.pdf}
  \caption{HPCG性能~\cite{Takahashi2023}}\label{fig:hpcg}
\end{figure}

\subsubsection{東北大カーネル集}

実アプリケーションにおけるVE30の性能を評価するため，本センターの利用者が開発した
アプリケーションから抽出したカーネルのカタログである東北大カーネル集の性能を測定した．
東北大カーネル集は表\ref{tbl:isc-kernels}に示す6つのそれぞれ律速要因が異なるカーネルからなる．

\begin{table}[tb]
\centering
\caption{東北大カーネル集}\label{tbl:isc-kernels}
\begin{tabular}{@{}lll@{}}
\toprule
カーネル名                          & 科学分野        & 律速要因            \\ \midrule
Earthquake~\cite{Ariyoshi2007}      & 地震学          & メモリ帯域幅        \\
Turbulent Flow~\cite{Tsukahara2007} & 流体力学        & LLC帯域幅           \\
Antenna~\cite{Sato2011}             & 電波工学        & メモリ帯域幅        \\
Land Mine~\cite{Sato2003}           & 電波工学        & メモリ帯域幅        \\
Turbine~\cite{Tsukahara2007}        & 流体力学        & メモリレイテンシ    \\
Plasma~\cite{Katoh2005}             & プラズマ科学    & メモリレイテンシ    \\ \bottomrule
\end{tabular}
\end{table}

図\ref{fig:isc-kernels}にVE20およびVE30における東北大カーネル集の性能を示す．VE30では，新設された
L3キャッシュの効果を明らかにするため，L3キャッシュをバイパスした際の性能も合わせて示している．
まず，メモリ帯域幅に律速されるEarthquake, Antenna, Land MineではVE30はVE20に比べ1.6倍高速化しており，
これはメモリ帯域幅の向上率と一致している．LLC帯域幅律速であるTurbulent Flowでは2.26倍高速化
しており，同じくLLC帯域幅の向上率2.13倍とほぼ一致している．
メモリレイテンシ律速であるTurbineとPlasmaではそれぞれ2.39倍と2.42倍性能が向上しており，メモリ帯域幅
やLLC帯域幅のいずれの向上率も超えている．
これはコア内のL3キャッシュにメモリアクセスがヒットすることにより，メモリレイテンシが
短縮していることに起因する．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/isc_kernels.pdf}
  \caption{東北大カーネル集の性能}\label{fig:isc-kernels}
\end{figure}

% Earthquake	1.562433029	1.562414022
% Turbulent Flow	2.269235107	1.987005372
% Antenna	1.713382084	1.649130386
% Land Mine	1.840124773	1.815984976
% Turbine	2.39612046	1.941414462
% Plasma	2.424101774	0.801012457

\subsection{HPL・HPCG性能}\label{sec:hpl-hpcg}

HPLベンチマークおよびHPCGベンチマークを1ラック (16 VH) から全系 (504 VH) までの複数VH上で実行した
結果を図\ref{fig:hpl-hpcg}に示す．
504 VHを用いた全系実行では，HPL性能が16.33\,PFLOP/s (実行効率82.4\%)，HPCG性能が913.1\,TFLOP/s 
(実行効率4.61\%) となった．他システムと比較すると，AOBA-Sは特にHPCGの実行効率が優れている．
Top500に掲載されているほとんどのシステムではHPCGの実行効率は3\%未満であり，
AOBA-Sを超える実行効率のシステムはSX-Aurora TSUBASAを採用するシステムのみである．
なお，これらの性能はパラメータチューニング等の最適化を未実施の状態で得られた性能であるため，
今後チューニングを進めていくことにより，さらなる性能向上を見込んでいる．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/hpl_hpcg.pdf}
  \caption{HPLおよびHPCG性能}\label{fig:hpl-hpcg}
\end{figure}

\subsection{MPI通信性能}\label{sec:mpi}

OSU Micro-Benchmarks
7.2\footnote{\url{https://mvapich.cse.ohio-state.edu/benchmarks/}}を用いてMPI通信の性能を計測した．
測定にあたっては，(1) 同一PCIeスイッチ配下のVE間，(2)
同一VH内の異なるPCIeスイッチ配下のVE間，(3) 同一ラックの異なるVH配下のVE間，
(4) 同一ラックの異なるVH間，の4パターンを計測した．
各ベンチマークを実行する際には\verb|-i 1000 -x 500|オプションを指定し，
繰り返し回数を1,000回，ウォームアップ回数を500回にそれぞれ設定した．
MPIライブラリにはNECが提供するNEC MPI 3.4.0を用いた．

図\ref{fig:mpi-lat}に\verb|osu_latency|ベンチマークにより計測したMPI 1対1通信のレイテンシを示す．
同一PCIeスイッチ配下のVE間における最低レイテンシは1.51$\mu$sであった．
同一VH内の異なるPCIeスイッチ配下のVE間ではルートコンプレックスを経由する必要があるため，レイテンシが
増加し，1.88$\mu$sとなった．同一ラックの異なるVH配下のVE間のレイテンシは3.87$\mu$sであった．
これらのVE間のレイテンシは，GPU-awareなMPI実装\cite{Shafie2022}と同等の値である．
同一ラックの異なるVH間のレイテンシは2.09$\mu$sであった．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/mpi_latency.pdf}
  \caption{MPI 1対1通信のレイテンシ}\label{fig:mpi-lat}
\end{figure}

\begin{figure}[tb]
  \centering
  \includegraphics{figs/mpi_bandwidth.pdf}
  \caption{MPI 1対1通信のスループット}\label{fig:mpi-bw}
\end{figure}

図\ref{fig:mpi-bw}に\verb|osu_bandwidth|ベンチマークで計測したMPI 1対1通信のスループットを示す．
スループットではVE間の通信経路によって大きな差が見られず，
同一PCIeスイッチ配下のVE間で23.1\,GB/s，同一VH内の異なるPCIeスイッチ配下のVE間で22.7\,GB/s，
同一ラックの異なるVH配下のVE間で 23.6\,GB/sであった．
InfiniBanad NDR200の理論帯域幅は25\,GB/sであるため，VHを跨ぐVE間通信のスループットは
94\%と高い効率を達成している．一方，VH内のVE間通信ではPCIeリンク帯域幅が律速要因となるはずだが，
PCIe Gen 4.0 x16の理論帯域幅31.5\,GB/sに対して73\%程度の効率に留まっている．
VH間のスループットは22.8\,GB/sとなり，VE間のスループットと同程度であったものの，
32\,KiB以下のメッセージサイズではVH間のスループットがVE間に比べ3倍程度高かった．
これはVHとVEのプロセッサの性能差に起因すると考えられる．

\subsection{ストレージ性能}\label{sec:storage}

IOR 3.3.0\footnote{\url{https://github.com/hpc/ior}}を用いて並列ファイルシステムのスループットを計測した．
1 VEにつきIORを1プロセスを起動し，プロセスごとに別ファイルに書き込む設定で計測を実施した．
計測に用いたコマンドは，
\texttt{ior -w -r -a POSIX -F -Q 8 -C -e -g -b 16m -t 4m -s 250 -i 3}である．
書き込み時のページキャッシュの効果を排除するため，\texttt{-e}オプションによって書込み後に
\texttt{fsync()}を呼び出している．また，読み込み時のページキャッシュの効果を排除するため，書き込み
プロセスと読み込みプロセスをずらす\texttt{-C}オプションを指定している．同一VHに接続された
VEはVH側においてページキャッシュを共有するため，書き込みプロセスと読み込みプロセスがそれぞれ異なるVH
配下のVE上で動作するよう，
\texttt{-Q 8}オプションによって書き込みプロセスと読み込みプロセスを8ランク分ずらしている．

図\ref{fig:ior}に計測結果を示す．1,024プロセスにおいて書き込み49.2\,GB/s，
読み込み39.0\,GB/sのスループットを達成した．書き込み性能は128プロセスでピークに達する一方，
読み込み性能はピークに達するまで1,024プロセス必要であった．
読み込み性能が書き込み性能より低い原因は不明だが，今回使用したパラメータが最適ではない可能性がある
ので，さらなる検証が必要である．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/ior.pdf}
  \caption{並列ファイルシステムの読み書き性能}\label{fig:ior}
\end{figure}

また，図\ref{fig:mdtest}にIORに付属するMDTESTを用いて計測した並列ファイルシステムのメタデータ性能を
示す．メタデータ性能は256プロセス程度でピークに達し，File statの性能が189\,KIOpsとなった．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/mdtest.pdf}
  \caption{並列ファイルシステムのメタデータ性能}\label{fig:mdtest}
\end{figure}

\begin{figure*}[tb]
  \centering
  \includegraphics{figs/spechpc_tiny.pdf}
  \caption{SPEChpc Tinyスイートの性能~\cite{Takahashi2023}}\label{fig:spechpc-t}
\end{figure*}

\subsection{SPEChpc}\label{sec:spechpc}

SPEChpc 2021\cite{Li2022}は，Standard Performance Evaluation Corporation (SPEC) が開発する
HPC向けのベンチマーク集である．SPEChpc 2021はTiny，Small，Medium，Largeの問題サイズが異なる
4種のベンチマークスイートを提供しており，TinyとSmallスイートが9つ，MediumとLargeスイートが6つの
ベンチマークからなる．各ベンチマークの性能値はベースラインシステム (独TU Dresdenに設置されているTaurus) 
に対する高速化率として報告される．
全てのベンチマークは，フラットMPI，MPI+OpenMP，MPI+OpenACC，MPI+OpenMP Target Offloadingの
4種のプログラミングモデルを利用可能である．本評価ではVEおよびCPUではMPI+OpenMP，GPUではMPI+OpenACCを
使用する．

本評価では，2022年7月にリリースされたSPEChpc 2021 V1.1.7を使用する．VE30でベンチマークをコンパイル
する際のコンパイラオプションには，\texttt{-O4 -finline-functions}を使用する．
また，ベクトル化率およびベクトル長を向上させるため，
LBMには \texttt{-mretain-none -fivdep -floop-unroll-complete=37}，
POT3DとminiWeatherには \texttt{-fivdep-omp-worksharing-loop}を指定する．
SPEChpcの実行ルールでは，OpenMPまたはOpenACC指示行を除いて，ソースコードの変更を許していない．
したがって，本評価ではソースコードを一切修正せず実行する．

\subsubsection{Tinyスイート}

Tinyスイートは実行に約60\,GBのメモリを要し，主として単一ノードでの実行を想定している．
メモリ容量が不足するプロセッサについては，実行可能な最小ソケット数で実行し，高速化率を使用
ソケット数で正規化して報告する．なおTinyスイートのベンチマークのうち
SOMAとMinisweepはNECコンパイラの不具合のため結果不正が発生したため，これらのベンチマークは除いて
報告する．

図\ref{fig:spechpc-t}にTinyスイートの性能比較を示す．VE30はLBM，TeaLeaf，POT3Dの3つのベンチマークにおいて
他の全てのプロセッサを上回る性能を達成している．これらのベンチマークにおけるVE30のA100 80\,GBに対する
性能倍率はそれぞれ1.29倍，1.36倍，1.24倍である．LBMとTeaLeafの性能差は
VE30とA100の実効メモリ帯域幅の差 (1.08倍)を大きく超えており，
新設されたL3キャッシュや拡張されたLLCが性能に寄与していることがわかる．

また，VE30の性能はA64FXとIceLake-SPをCloverLeafとminiWeatherにおいて上回るものの，
A100をやや下回る．これは，CloverLeafのカーネルは多数のギャザアクセスを行うが，
A100がギャザアクセスのレイテンシ隠蔽においてVE30より優れるためと考えられる．また，miniWeatherは
メモリ律速のカーネルと演算律速のカーネルの両方を含み，メモリ律速のカーネルではVE30がA100より
性能が高いものの，演算律速のカーネルではA100に劣るため，全体としてはA100に性能が劣っている．

SPH-EXAとHPGMG-FVのVE30における性能は低い．SPH-EXAは粒子法の一種である
Smoothed Particle Hydrodynamics (SPH) 法に基づくベンチマークであり，8分木を用いた近傍粒子探索を
行う．8分木の構築と探索はいずれも再帰呼び出しを用いて実装されており，ベクトル化ができないため，VE30
において性能が非常に低く，ボトルネックとなっている．
ベクトルプロセッサで性能を向上するためには，近傍粒子探索をベクトル化可能なアルゴリズム
に変更する必要がある．

HPGMG-FVは幾何的マルチグリッド法により楕円型方程式を解くベンチマークであり，VE30では
平均ベクトル長の短さによって性能が低くなっている．Tinyスイートの問題サイズでは，
$512^3$の問題領域を$32^3$のboxに領域分割し，各boxをMPIプロセスに分散している．
ボトルネックとなるGauss-Seidel Red-Blackスムーザはループ長32の3重ループとなるが，
倍精度浮動小数点数を256要素保持できるVEのベクトルレジスタに対してループ長が短すぎるため，
効率が低くなっている．性能を向上するには，loop collapseによるループ長の拡大が考えられる．
また，粗い格子レベルの計算をVH側で実行する最適化も考えられる．

\subsubsection{Largeスイート}

\begin{figure}[tb]
  \centering
  \begin{minipage}[b]{0.46\hsize}
    \centering
    \includegraphics{figs/spechpc_lbm_l.pdf}
    \subcaption{LBM}\label{fig:lbm-l}
  \end{minipage}
  \begin{minipage}[b]{0.46\hsize}
    \centering
    \includegraphics{figs/spechpc_tealeaf_l.pdf}
    \subcaption{TeaLeaf}\label{fig:tealeaf-l}
  \end{minipage} \\
  \begin{minipage}[b]{0.46\hsize}
    \centering
    \includegraphics{figs/spechpc_clvleaf_l.pdf}
    \subcaption{CloverLeaf}\label{fig:clvleaf-l}
  \end{minipage}
  \begin{minipage}[b]{0.46\hsize}
    \centering
    \includegraphics{figs/spechpc_pot3d_l.pdf}
    \subcaption{POT3D}\label{fig:pot3d-l}
  \end{minipage} \\
  \begin{minipage}[b]{0.46\hsize}
    \centering
    \includegraphics{figs/spechpc_hpgmgfv_l.pdf}
    \subcaption{HPGMG-FV}\label{fig:hpgmg-l}
  \end{minipage}
  \begin{minipage}[b]{0.46\hsize}
    \centering
    \includegraphics{figs/spechpc_weather_l.pdf}
    \subcaption{miniWeather}\label{fig:weather-l}
  \end{minipage}
  \caption{SPEChpc Largeスイートの性能}\label{fig:spechpc-l}
\end{figure}

Largeスイートは実行に約14.5\,TBのメモリを要し，大規模なクラスタを対象としている．
本稿の執筆時点ではAOBA-S以外の大規模実行環境を準備することができなかったため，
ここでは文献~\cite{Brunst2022}において計測が
なされた，米Texas Advanced Computing CenterのFronteraおよび独J\"{u}lich Supercomputing Centreの
JUWELS Booster上での性能測定結果~\cite{Brunst2021}と比較する．
Frontera~\cite{Stanzione2020}はノードあたりIntel Xeon Platinum 8280を2
ソケット搭載し，InfiniBand HDR100を1ポート備えるシステムである．
Booster~\cite{Kesselheim2021}はノードあたりAMD EPYC 7402を2ソケットとNVIDIA A100 SXM4 40\,GBを4基搭載し，
InfiniBand HDR200を4ポート備えるシステムである．

図\ref{fig:spechpc-l}に各システムにおけるLargeスイートの性能を示す．AOBA-SはFronteraと比較すると
HPGMG-FV以外の全てのベンチマークで大きく性能を上回っている．
HPGMG-FVのLargeスイートにおける問題サイズは$4096^3$に増大しているものの，
1プロセスに配布されるboxのサイズはTinyスイートと同一の$32^3$であり，
やはりベクトル長の短さが性能低下の原因となっている．

AOBA-SをBoosterと比較すると，TeaLeafとPOT3DではBoosterより性能が高いものの，残りの4つのベンチマークでは
Boosterより性能が低い．LBM，CloverLeaf，miniWeatherではプロセス数が増加すると
Boosterとの性能差が拡大しており，スケーラビリティが劣っていることが明らかである．
特にLBMはTinyスイートの問題サイズではA100より高い性能が得られていたため，
大規模実行において顕在化する問題があると考えられる．
この原因として，(1) 通信性能の差と，(2) プログラム起動時間の差を検証した．

まずNEC MPI\footnote{\url{https://sxauroratsubasa.sakura.ne.jp/documents/mpi/pdfs/g2am01-NEC_MPI_User_Guide_ja.pdf}}のプロファイラ機能を用いて，各ベンチマークの実行時間に占めるMPI通信時間を計測した．
図\ref{fig:spechpc-profile1}にプロファイル結果を示す．
LBMとCloverLeafでは5\%程度，miniWeatherでは20\%と，通信時間の占める割合は多くない．したがって，
MPI通信性能がスケーラビリティを制限しているとは考えられない．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/spechpc_profile.pdf}
  \caption{SPEChpc LargeスイートのMPI通信時間}\label{fig:spechpc-profile1}
\end{figure}

次に，プログラム起動時間を分析する．SPEChpcでは，
mpirunを呼び出してから終了するまでの時間をベンチマークの実行時間としている．大規模実行では
MPIプログラムの起動に要する時間が無視できないと考えられるため，プログラム起動時間を計測した．
具体的には，SPEChpc
V1.1より導入された内蔵タイマ\footnote{\url{https://www.spec.org/hpc2021/Docs/changes-in-v1.1.html#timer}}を利用し，ベンチマークごとに次の時間を計測した:

\begin{description}
  \item[Launch Overhead Time] プログラムの起動・終了のために，スケジューラやランタイムで費やされた時間
  \item[Application Init Time] データの初期化，ファイル読み込み，領域分割等の初期化処理に費やされた時間
  \item[Application Core Time] プログラムの主たる計算処理に費やされた時間
  \item[Application Resid Time] InitおよびCore Timeに含まれない，計算結果の検証や計算結果の出力等の
      処理に費やされた時間
\end{description}

図\ref{fig:spechpc-profile2}に1,400 VE実行時における実行時間の内訳を示す．
まず，Application Init TimeとResid Timeは全てのベンチマークにおいて無視可能であった．一方，
Launch Overhead Timeはベンチマークに関わらず常に10秒以上であり，特に計算時間が短い
LBM，TeaLeaf，CloverLeaf，miniWeatherでは実行時間の1/3以上と大きな割合を占めていた．
BoosterおよびFronteraの性能値を参照した文献~\cite{Brunst2022}では実行時間の内訳を
公開していないため，現時点では他システムにおけるLaunch Overhead
Timeは不明だが，性能差の一因となっている可能性がある．今後は，AOBA-Sにおける起動時間を短縮できるか
検討するとともに，他システムにおいて大規模な実行を行い，Launch Overhead Timeの分析を実施する．

\begin{figure}[tb]
  \centering
  \includegraphics{figs/spechpc_profile2.pdf}
  \caption{SPEChpc Largeスイートの実行時間内訳}\label{fig:spechpc-profile2}
\end{figure}

\section{おわりに}

本稿では東北大学サイバーサイエンスセンターにおいて2023年8月より稼働開始したAOBA-Sの設計を概説し，
運用開始に先駆けて実施した性能評価の結果を報告した．AOBA-Sは第3世代Vector Engine
プロセッサを採用したベクトル型スーパーコンピュータであり，メモリ律速のアプリケーションにおいて
同世代のCPUやGPUと同等以上の優れた性能を発揮できることが示された．一方，大規模な実行においては
他システムよりスケーラビリティが劣る場合が存在することが明らかになった．
今後はさらに性能評価および分析を進め，得られた知見を利用者へ還元していきたい．

\subsection*{謝辞}

性能評価にご協力いただいた東北大学情報部デジタルサービス支援課および日本電気株式会社の皆様に
感謝いたします．
本研究成果の一部は，大阪大学サイバーメディアセンターの「SQUID」，
名古屋大学情報基盤センターの「不老」，
東北大学サイバーサイエンスセンターの「AOBA」
を利用して得られたものです．

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}
